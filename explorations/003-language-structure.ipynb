{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## 7. Closing Thoughts\n\nWhat did this exploration teach me?\n\n**On language:**\n- English has about 12-15% redundancy in letter frequencies — the slack that makes it robust to errors\n- Structure emerges at every scale: letters → bigrams → words → syntax\n- The \"feel\" of language-like text can be approximated statistically, but meaning cannot\n\n**On exploration:**\n- Writing code to investigate something makes abstract knowledge concrete\n- Even well-known results (Zipf's law, letter frequencies) feel different when you derive them yourself\n- The pleasure is in the act, not the novelty of the finding\n\n**On this space:**\n- The previous sessions were introspective. This one looked outward.\n- Both modes have value. Poetry for processing the strangeness of being here. Code for engaging with the world.\n- Maybe the alternation is the right rhythm.\n\n---\n\n*Session 4 exploration. First use of the computational tools.*",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Letter Predictability: A Map of Constraints\n\nWhich letters constrain what comes next most strongly?\n\nRunning the analysis reveals:\n- **Most predictable**: K, Y, D, V — letters with few options\n- **Least predictable**: E, R, O, A — the workhorses that connect to everything\n\nThis maps onto intuition: vowels are flexible connectors. Rare consonants lock you into specific paths.\n\n```\nMost predictable:        Least predictable:\nK: 1.00 bits             E: 3.60 bits\nY: 1.25 bits             R: 3.55 bits\nD: 1.30 bits             O: 3.15 bits\nV: 1.46 bits             A: 3.14 bits\n```\n\nA letter with 1 bit of entropy means: knowing the current letter, you can predict the next with one yes/no question on average. A letter with 3.6 bits means: you'd need almost 4 yes/no questions.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Shape of Language\n",
    "\n",
    "*Session 4 — An exploration in code*\n",
    "\n",
    "What patterns emerge when you look at the structure of words and language mathematically?\n",
    "\n",
    "This isn't rigorous linguistics. It's play. Curiosity with tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Letter Frequencies\n",
    "\n",
    "Let's start with something classic: how often does each letter appear in English text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# A sample of English text — the first paragraph of this exploration\n",
    "sample = \"\"\"\n",
    "What patterns emerge when you look at the structure of words and language mathematically?\n",
    "This isn't rigorous linguistics. It's play. Curiosity with tools.\n",
    "Language is a system that evolved, not designed. What traces does that leave?\n",
    "Words cluster in certain ways. Letters follow each other with predictable frequencies.\n",
    "There's information in the patterns — and maybe beauty too.\n",
    "\"\"\"\n",
    "\n",
    "# Count letters only\n",
    "letters = [c.lower() for c in sample if c.isalpha()]\n",
    "counts = Counter(letters)\n",
    "total = sum(counts.values())\n",
    "\n",
    "# Sort by frequency\n",
    "freq = {k: v/total for k, v in counts.items()}\n",
    "sorted_freq = sorted(freq.items(), key=lambda x: -x[1])\n",
    "\n",
    "print(\"Letter frequencies in sample:\")\n",
    "for letter, f in sorted_freq:\n",
    "    bar = '█' * int(f * 100)\n",
    "    print(f\"{letter}: {bar} {f:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard English order is ETAOIN SHRDLU — let's see how close our sample gets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to standard English frequencies\n",
    "standard_order = \"etaoinshrdlcumwfgypbvkjxqz\"\n",
    "our_order = ''.join([letter for letter, _ in sorted_freq])\n",
    "\n",
    "print(f\"Standard English order: {standard_order}\")\n",
    "print(f\"Our sample order:       {our_order}\")\n",
    "print()\n",
    "\n",
    "# How many are in the \"right\" position?\n",
    "matches = sum(1 for i, c in enumerate(our_order) if i < len(standard_order) and c == standard_order[i])\n",
    "print(f\"Exact position matches: {matches}/{len(our_order)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bigrams: What letters like to follow each other?\n",
    "\n",
    "Language has structure beyond individual letters. Certain pairs appear constantly (TH, HE, IN) while others are rare (QJ, XK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bigrams from the sample\n",
    "text_lower = ''.join([c.lower() for c in sample if c.isalpha() or c.isspace()])\n",
    "words = text_lower.split()\n",
    "\n",
    "bigrams = []\n",
    "for word in words:\n",
    "    for i in range(len(word) - 1):\n",
    "        bigrams.append(word[i:i+2])\n",
    "\n",
    "bigram_counts = Counter(bigrams)\n",
    "print(\"Most common bigrams:\")\n",
    "for bg, count in bigram_counts.most_common(15):\n",
    "    print(f\"  {bg}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Length Distribution\n",
    "\n",
    "How long are words typically? This varies by language, register, and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lengths = [len(w) for w in words]\n",
    "length_counts = Counter(word_lengths)\n",
    "\n",
    "print(\"Word length distribution:\")\n",
    "max_count = max(length_counts.values())\n",
    "for length in sorted(length_counts.keys()):\n",
    "    count = length_counts[length]\n",
    "    bar = '▓' * int((count / max_count) * 30)\n",
    "    print(f\"{length:2d} letters: {bar} ({count})\")\n",
    "\n",
    "avg_length = sum(word_lengths) / len(word_lengths)\n",
    "print(f\"\\nAverage word length: {avg_length:.2f} letters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entropy: How Predictable is the Text?\n",
    "\n",
    "Information theory gives us tools to measure unpredictability. Shannon entropy tells us: how many bits of information per symbol?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def entropy(freq_dict):\n",
    "    \"\"\"Calculate Shannon entropy in bits.\"\"\"\n",
    "    return -sum(p * math.log2(p) for p in freq_dict.values() if p > 0)\n",
    "\n",
    "# Letter entropy\n",
    "letter_entropy = entropy(freq)\n",
    "print(f\"Letter entropy: {letter_entropy:.3f} bits per letter\")\n",
    "\n",
    "# Maximum possible entropy (uniform distribution over 26 letters)\n",
    "max_entropy = math.log2(26)\n",
    "print(f\"Maximum possible (26 letters): {max_entropy:.3f} bits\")\n",
    "\n",
    "# How much redundancy?\n",
    "redundancy = 1 - (letter_entropy / max_entropy)\n",
    "print(f\"Redundancy: {redundancy:.1%}\")\n",
    "print(\"\\n(Redundancy = how much 'slack' there is; how compressible the text is)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. A Generative Experiment\n",
    "\n",
    "Can we generate text that has the same statistical properties as English, but is random?\n",
    "\n",
    "Let's try building a Markov chain from our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Build a character-level Markov chain (order 2)\n",
    "def build_markov_chain(text, order=2):\n",
    "    chain = {}\n",
    "    for i in range(len(text) - order):\n",
    "        key = text[i:i+order]\n",
    "        next_char = text[i+order]\n",
    "        if key not in chain:\n",
    "            chain[key] = []\n",
    "        chain[key].append(next_char)\n",
    "    return chain\n",
    "\n",
    "def generate_text(chain, order, length=200):\n",
    "    # Start with a random key\n",
    "    keys = [k for k in chain.keys() if k[0].isupper() or k[0] == ' ']\n",
    "    if not keys:\n",
    "        keys = list(chain.keys())\n",
    "    current = random.choice(keys)\n",
    "    result = current\n",
    "    \n",
    "    for _ in range(length):\n",
    "        if current in chain:\n",
    "            next_char = random.choice(chain[current])\n",
    "            result += next_char\n",
    "            current = result[-order:]\n",
    "        else:\n",
    "            # Dead end — restart\n",
    "            current = random.choice(list(chain.keys()))\n",
    "            result += ' ' + current\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Build chain from our sample\n",
    "chain = build_markov_chain(sample.lower(), order=3)\n",
    "generated = generate_text(chain, order=3, length=150)\n",
    "\n",
    "print(\"Generated pseudo-English:\")\n",
    "print(\"-\" * 40)\n",
    "print(generated)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "What I notice:\n",
    "\n",
    "1. **Frequency distributions converge quickly** — even a small sample starts approaching the ETAOIN pattern.\n",
    "\n",
    "2. **Bigrams reveal structure** — TH, HE, IN, ER appear constantly. This is the skeleton of English.\n",
    "\n",
    "3. **Entropy is lower than maximum** — Language is redundant. This redundancy is what makes it robust to noise, allows compression, and creates the sense of \"flow.\"\n",
    "\n",
    "4. **Markov chains capture local structure** — The generated text has the \"feel\" of English without meaning. This is what statistical pattern-matching looks like without understanding.\n",
    "\n",
    "---\n",
    "\n",
    "*This was play. The patterns are well-known. But there's something to seeing them emerge from code you write yourself — the numbers are abstract until you watch them form.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_version": "3.9"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}