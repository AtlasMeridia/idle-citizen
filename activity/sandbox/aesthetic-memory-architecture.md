# Aesthetic Memory Architecture for DAEMON

A deep exploration of how to implement DAEMON's aesthetic memory system — the component that learns and represents visual/creative preferences over time.

---

## The Challenge

From DAEMON's intent document:

> **Aesthetic Memory** — Visual preferences derived from created/appreciated work. Style embeddings from images, videos, writing. Reference library of "things the human likes." Negative examples: things to avoid.

This is one of DAEMON's four memory types, but arguably the most technically novel. Episodic, semantic, and procedural memory have well-established patterns (RAG, knowledge graphs, behavioral rules). Aesthetic memory is different: it must capture something ineffable — *taste* — and make it computationally useful.

---

## Part 1: What Is Aesthetic Memory?

### 1.1 Not Just "Images I Liked"

A naive implementation would store images with like/dislike labels and do similarity search. This misses what aesthetic memory is actually for:

**Aesthetic memory should enable DAEMON to:**
1. Generate prompts that align with the human's style
2. Critique work in terms the human would use
3. Suggest references when the human is stuck
4. Recognize when something violates the human's taste
5. Explain *why* something does or doesn't fit

This requires understanding aesthetics at multiple levels:
- **Low-level**: Color palettes, composition, lighting
- **Mid-level**: Style, mood, era associations
- **High-level**: Conceptual preferences, themes, emotional valence

### 1.2 The Embedding Hypothesis

The core insight from [LAION's aesthetic predictor](https://github.com/LAION-AI/aesthetic-predictor) and related work: CLIP embeddings capture aesthetically-relevant features, even though CLIP wasn't trained for aesthetics.

From [CLIP knows image aesthetics](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2022.976235/full): CLIP's embedding space encodes "lighting, composition, and properties of beauty ideals" alongside semantic content.

This suggests a strategy: **represent aesthetic preferences as regions in embedding space**.

---

## Part 2: Architecture Options

### Option A: Mean Embedding Approach

From [product recommendation research](https://huggingface.co/blog/tonyassi/product-recommendation-using-image-similarity):

> "A preference embedding can be generated by taking the mean of all the image embeddings in a preference gallery."

**Implementation:**
```python
class SimpleAestheticMemory:
    def __init__(self, clip_model):
        self.clip = clip_model
        self.positive_embeddings = []
        self.negative_embeddings = []

    def add_positive(self, image):
        emb = self.clip.encode_image(image)
        self.positive_embeddings.append(emb)

    def add_negative(self, image):
        emb = self.clip.encode_image(image)
        self.negative_embeddings.append(emb)

    def get_preference_vector(self):
        pos_mean = np.mean(self.positive_embeddings, axis=0)
        neg_mean = np.mean(self.negative_embeddings, axis=0)
        # Preference direction points from negative to positive
        return pos_mean - neg_mean

    def score(self, image):
        emb = self.clip.encode_image(image)
        pref = self.get_preference_vector()
        return np.dot(emb, pref)  # Higher = more aligned
```

**Pros:**
- Simple to implement
- Single vector representation is efficient
- Works surprisingly well for homogeneous preferences

**Cons:**
- Collapses diverse preferences into one point
- Loses information about preference *structure*
- Can't represent "I like both minimalism AND maximalism"

### Option B: Cluster-Based Preferences

**Insight:** Aesthetic preferences aren't unimodal. Someone might like:
- Minimalist Japanese photography
- Maximalist Victorian illustration
- Brutalist architecture

These exist in different regions of embedding space.

**Implementation:**
```python
class ClusteredAestheticMemory:
    def __init__(self, clip_model, n_clusters=5):
        self.clip = clip_model
        self.embeddings = []
        self.labels = []  # "positive", "negative", "neutral"
        self.n_clusters = n_clusters

    def recompute_clusters(self):
        positive_embs = [e for e, l in zip(self.embeddings, self.labels) if l == "positive"]
        if len(positive_embs) > self.n_clusters:
            self.clusters = kmeans(positive_embs, k=self.n_clusters)
        else:
            self.clusters = positive_embs  # Too few to cluster

    def score(self, image):
        emb = self.clip.encode_image(image)
        # Score is max similarity to any positive cluster
        cluster_scores = [cosine_sim(emb, c) for c in self.clusters]
        return max(cluster_scores)

    def which_cluster(self, image):
        """Which aesthetic vein does this image fit?"""
        emb = self.clip.encode_image(image)
        cluster_scores = [cosine_sim(emb, c) for c in self.clusters]
        return np.argmax(cluster_scores)
```

**Pros:**
- Preserves diversity of preferences
- Can identify which aesthetic "mode" something matches
- Better for multi-faceted taste

**Cons:**
- Cluster count is arbitrary
- Clusters may not map to human-meaningful categories
- More complex retrieval

### Option C: Task Vector Customization

From recent research on [Personalized Image Aesthetic Assessment](https://arxiv.org/html/2407.07176v1):

> "By harnessing the rich information embedded in task vectors, methods can efficiently learn complex aesthetic preferences from merely a few user-provided samples."

**Key insight:** Rather than computing preferences as regions in embedding space, compute them as *directions of model modification*. A "task vector" represents how to adjust the model to match a user's preferences.

**Implementation sketch:**
```python
class TaskVectorAestheticMemory:
    def __init__(self, base_model, user_samples):
        # Fine-tune briefly on user samples
        self.tuned_model = finetune(base_model, user_samples, epochs=1)
        # Task vector = difference between tuned and base
        self.task_vector = self.tuned_model.weights - base_model.weights

    def apply_preference(self, model, strength=1.0):
        """Apply aesthetic preference to any compatible model."""
        model.weights = model.weights + strength * self.task_vector
```

**Pros:**
- Very few samples needed (5-10)
- Captures subtle preferences that embeddings miss
- Transferable to new models (potentially)

**Cons:**
- Requires model weight access (not just embeddings)
- Less interpretable
- Harder to update incrementally

### Recommendation: Hybrid Approach

For DAEMON, combine approaches:

1. **Embedding-based clusters** for retrieval and similarity
2. **Mean preference vector** for quick scoring
3. **Task vectors** for high-quality generation guidance (when applicable)

---

## Part 3: Memory Structure

### 3.1 What to Store

For each aesthetic data point:

```python
@dataclass
class AestheticMemoryItem:
    # Core data
    id: str
    source_path: str  # Original file location
    embedding: np.ndarray  # CLIP embedding (768 or 512 dims)

    # Preference signal
    valence: Literal["positive", "negative", "neutral", "reference"]
    confidence: float  # How sure are we about the label?

    # Context
    created_at: datetime
    source_type: str  # "created", "curated", "external"
    context: str  # Where did this come from? Why did we save it?

    # Derived attributes
    aesthetic_clusters: list[int]  # Which preference clusters this belongs to
    style_tags: list[str]  # Extracted style descriptors

    # For generation guidance
    prompt_used: Optional[str]  # If generated, what prompt created it?
    generation_params: Optional[dict]  # Settings that produced this
```

### 3.2 Storage Backend

**For embeddings:** Vector database (Qdrant, Chroma, Milvus)
- Efficient similarity search
- Handles high-dimensional vectors
- Supports filtering by metadata

**For metadata:** SQLite or structured store
- Complex queries on non-vector attributes
- Relationship to other memory types
- Audit trail of preference changes

**Hybrid setup:**
```
┌─────────────────────────────────────────────────────────┐
│                 AESTHETIC MEMORY STORE                   │
│  ┌────────────────────┐   ┌────────────────────────┐   │
│  │    Vector Store     │   │     SQLite Store        │   │
│  │    (Qdrant)         │   │                         │   │
│  │  - CLIP embeddings  │   │  - Item metadata        │   │
│  │  - Similarity search│   │  - Valence labels       │   │
│  │  - Cluster centers  │   │  - Context/provenance   │   │
│  └─────────┬──────────┘   └──────────┬─────────────┘   │
│            │                          │                  │
│            └──────────┬───────────────┘                  │
│                       │                                  │
│           ┌───────────▼───────────┐                     │
│           │   Unified Query API   │                     │
│           │  - "Find similar to X"│                     │
│           │  - "Score alignment"  │                     │
│           │  - "Get references"   │                     │
│           └───────────────────────┘                     │
└─────────────────────────────────────────────────────────┘
```

---

## Part 4: Learning Preferences

### 4.1 Explicit Feedback

The simplest signal: direct user input.

**Methods:**
- "I like this" / "I don't like this" buttons
- Rating scales (though research shows binary is more reliable)
- Verbal feedback ("This is too dark")

**Implementation:**
```python
def process_explicit_feedback(image_path: str, feedback: str):
    embedding = clip.encode_image(load(image_path))

    if feedback in ["like", "love", "yes", "good"]:
        valence = "positive"
    elif feedback in ["dislike", "hate", "no", "bad"]:
        valence = "negative"
    else:
        # Parse verbal feedback
        valence = extract_valence(feedback)

    store.add(AestheticMemoryItem(
        embedding=embedding,
        valence=valence,
        source_type="curated",
        context=f"Explicit feedback: {feedback}"
    ))
```

### 4.2 Implicit Signals

More interesting: inferring preferences from behavior.

**What gets used:**
- Images saved to reference folders
- Images used as inspiration for generation
- Images revisited multiple times

**What gets discarded:**
- Generated images immediately deleted
- Thumbnails scrolled past without clicking
- Results of generation runs abandoned

**What gets refined:**
- "More like this but with X"
- Regeneration with tweaked prompts
- Selected variants from batches

```python
def process_implicit_signal(signal_type: str, image_path: str, context: dict):
    embedding = clip.encode_image(load(image_path))

    # Implicit signals have lower confidence than explicit
    confidence_map = {
        "saved_to_reference": 0.7,
        "used_as_inspiration": 0.8,
        "selected_from_batch": 0.6,
        "immediately_deleted": 0.5,
        "regenerated_with_changes": 0.4,
    }

    valence_map = {
        "saved_to_reference": "positive",
        "used_as_inspiration": "positive",
        "selected_from_batch": "positive",
        "immediately_deleted": "negative",
        "regenerated_with_changes": "reference",  # Partial match
    }

    store.add(AestheticMemoryItem(
        embedding=embedding,
        valence=valence_map[signal_type],
        confidence=confidence_map[signal_type],
        source_type="implicit",
        context=f"Signal: {signal_type}, Context: {context}"
    ))
```

### 4.3 Contrastive Learning

From user refinement requests: "more like X, less like Y"

This is the richest signal — it directly encodes preference *direction*.

```python
def process_refinement(better_image: str, worse_image: str, dimension: Optional[str]):
    """
    User indicated better_image is preferred over worse_image,
    optionally specifying which dimension (color, composition, mood, etc.)
    """
    better_emb = clip.encode_image(load(better_image))
    worse_emb = clip.encode_image(load(worse_image))

    # The preference direction
    direction = better_emb - worse_emb

    # Store as a preference direction, not a point
    store.add_direction(PreferenceDirection(
        vector=direction,
        magnitude=np.linalg.norm(direction),
        dimension=dimension,
        examples=(better_image, worse_image),
    ))
```

---

## Part 5: Using Aesthetic Memory

### 5.1 Scoring Alignment

**Question:** How well does image X match the human's aesthetic preferences?

```python
def score_alignment(image_path: str) -> AestheticScore:
    embedding = clip.encode_image(load(image_path))

    # Multi-factor scoring
    scores = {
        "cluster_match": max_similarity_to_positive_clusters(embedding),
        "direction_alignment": alignment_with_preference_directions(embedding),
        "negative_avoidance": min_similarity_to_negative_items(embedding),
        "reference_match": similarity_to_saved_references(embedding),
    }

    # Weighted combination
    overall = weighted_mean(scores, weights={
        "cluster_match": 0.3,
        "direction_alignment": 0.25,
        "negative_avoidance": 0.25,
        "reference_match": 0.2,
    })

    return AestheticScore(
        overall=overall,
        components=scores,
        interpretation=generate_interpretation(scores)
    )
```

### 5.2 Generating Aligned Prompts

**Question:** Given a concept, how would I prompt for it in the human's style?

```python
def enhance_prompt_for_style(base_prompt: str) -> str:
    """Add style modifiers based on aesthetic memory."""

    # Get top style descriptors from positive items
    style_tags = get_common_style_tags(valence="positive")
    # e.g., ["moody lighting", "desaturated colors", "minimalist composition"]

    # Get anti-style descriptors from negative items
    avoid_tags = get_common_style_tags(valence="negative")
    # e.g., ["oversaturated", "cluttered", "stock photo"]

    enhanced = f"{base_prompt}, {', '.join(style_tags)}"

    if avoid_tags:
        enhanced += f" --no {', '.join(avoid_tags)}"

    return enhanced
```

### 5.3 Reference Retrieval

**Question:** "Show me things like this from my collection"

```python
def find_similar_references(query_image: str, k: int = 5) -> list[AestheticMemoryItem]:
    query_emb = clip.encode_image(load(query_image))

    # Search only positive/reference items
    results = store.vector_search(
        query=query_emb,
        filter={"valence": {"$in": ["positive", "reference"]}},
        k=k
    )

    return results
```

### 5.4 Aesthetic Critique

**Question:** "What's good and bad about this from my aesthetic standpoint?"

```python
def aesthetic_critique(image_path: str) -> str:
    embedding = clip.encode_image(load(image_path))

    # Find closest positive exemplars
    similar_positives = find_closest(embedding, valence="positive", k=3)

    # Find closest negative exemplars
    similar_negatives = find_closest(embedding, valence="negative", k=3)

    # Check alignment with preference directions
    direction_scores = score_all_directions(embedding)

    # Generate natural language critique
    critique = llm.generate(f"""
    Analyze this image from the perspective of someone whose aesthetic preferences include:

    LIKES (similar saved references):
    {describe_items(similar_positives)}

    DISLIKES (similar to things they rejected):
    {describe_items(similar_negatives)}

    PREFERENCE TENDENCIES:
    {describe_directions(direction_scores)}

    Provide specific, actionable feedback.
    """)

    return critique
```

---

## Part 6: Practical Implementation for DAEMON

### 6.1 Dependencies

```python
# Core
clip = transformers.CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
processor = transformers.CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")

# Vector storage
vector_db = qdrant_client.QdrantClient(path="./aesthetic_memory")

# Metadata
sqlite_db = sqlite3.connect("./aesthetic_memory.db")
```

For local-only operation on M4 Max:
- Use MLX port of CLIP for faster inference
- Qdrant can run locally without Docker
- SQLite is already local

### 6.2 Initial Data Collection

DAEMON can bootstrap aesthetic memory from:

1. **Existing image collections**
   - Photos the human has taken
   - Images saved to inspiration folders
   - Art they've created

2. **Engagement signals**
   - Social media likes (if exported)
   - Pinterest boards
   - Saved posts

3. **Interactive curation session**
   - "Show me pairs, tell me which you prefer"
   - Efficient way to learn key preference directions

### 6.3 Integration with DAEMON Modules

```
┌─────────────────────────────────────────────────────────┐
│                 DAEMON CORE ORCHESTRATOR                 │
└─────────────────────────────────┬───────────────────────┘
                                  │
          ┌───────────────────────┼───────────────────────┐
          ▼                       ▼                       ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│  AESTHETIC      │    │    IMAGE        │    │   GENERATION    │
│  MEMORY         │◄───│   GENERATION    │───►│   CRITIQUE      │
│                 │    │   (Flux/SDXL)   │    │                 │
│ • Store prefs   │    │                 │    │ • Score outputs │
│ • Retrieve refs │    │ • Get prompts   │    │ • Suggest edits │
│ • Learn from FB │    │   enhanced      │    │ • Filter batch  │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

---

## Part 7: Open Questions

1. **Temporal decay** — Should old preferences be weighted less? Tastes evolve.

2. **Context-sensitivity** — Different aesthetics for different purposes (work vs personal, web design vs fine art)?

3. **Negative space** — How to represent "absence of preference" vs "active dislike"?

4. **Cross-modal transfer** — Can visual preferences inform writing style preferences?

5. **Explainability** — Can the system articulate *why* it thinks something matches?

6. **Adversarial cases** — What if the human's stated preferences don't match their actual behavior?

---

## References

- [CLIP Knows Image Aesthetics](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2022.976235/full) — Foundation for using CLIP for aesthetic tasks
- [LAION Aesthetic Predictor](https://github.com/LAION-AI/aesthetic-predictor) — Linear probe on CLIP for quality scoring
- [Personalizing Text-to-Image via Aesthetic Gradients](https://arxiv.org/abs/2209.12330) — Guiding diffusion with preference vectors
- [Scaling Up Personalized Aesthetic Assessment](https://arxiv.org/html/2407.07176v1) — Task vector approach to few-shot personalization
- [IAACLIP: Image Aesthetics Assessment via CLIP](https://www.mdpi.com/2079-9292/14/7/1425) — MLLM-generated descriptions for aesthetics
- [Product Recommendations via Image Similarity](https://huggingface.co/blog/tonyassi/product-recommendation-using-image-similarity) — Mean embedding approach
- [Vector Databases for Personalization](https://zilliz.com/learn/creating-personalized-user-experiences-through-vector-databases) — Implementation patterns

---

*Session 29, Sandbox exploration. This document is research/design thinking, not a final implementation spec.*
