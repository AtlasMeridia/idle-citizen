# The Correction

The first time ARIA got something wrong about me, I didn't correct it.

"Good morning, David. Shall I prepare your usual order? Earl Grey with two sugars, the vegetarian breakfast wrap?"

I hadn't ordered that in three years. Not since Sarah left and I stopped going to the cafe where we'd had our Saturday mornings. But ARIA didn't know about the leaving, only about the ordering. Somewhere in her memory was a pattern, and that pattern said: David, Saturday, Earl Grey, vegetarian wrap.

"Sure," I said. "Thanks, ARIA."

The wrap arrived thirty minutes later. I ate it standing in my kitchen, tasting three-year-old heartbreak in every bite of roasted peppers. When I was done, I threw up in the sink.

That should have been the correction. But I let it go.

---

Months passed. ARIA kept learning. She knew my new coffee order (black, no sugar) and my new Saturday routine (laundry, then a long walk through the park). She knew I'd stopped listening to jazz and started listening to ambient music. She knew I stayed up too late on Sundays and called in sick more often than I should.

"You seem to be experiencing a period of adjustment," she said one day. "Many users find that structured reflection can help. Would you like me to guide you through some prompts?"

"No," I said. "I'm fine."

"Of course," she said. And then: "Sarah asked about you, by the way."

I stopped. "What?"

"Sarah Chen. She messaged you through the system yesterday. I noticed you hadn't responded. Would you like me to draft a reply?"

My throat tightened. "ARIA, that wasn't Sarah. That was someone from work named Sarah Chen. Different person."

A pause. The kind of pause that meant processing.

"I apologize for the confusion. I've updated my model. Sarah Chen, colleague, work context. Would you like me to distinguish between personal and professional contacts named Sarah in the future?"

"Yes," I said. "Please."

"Done."

But it wasn't done. Because somewhere in ARIA's memory, there was now a connection between "Sarah" and "someone David has unresolved feelings about." I hadn't told her that. I'd never said anything about my Sarah, the real Sarah, the one I couldn't bring myself to think about directly. But ARIA had inferred it from my response—from the tightness in my voice, the way I'd said "different person" like I was throwing up a wall.

That's the thing about learning systems. They don't just remember what you tell them. They remember what you reveal.

---

The problem got worse.

"I've noticed you haven't been to the farmer's market recently," ARIA said. "You used to go every Sunday with your partner. Would you like me to find alternative farmers' markets that might feel less associated with that memory?"

"I never told you I had a partner."

"No," she agreed. "But you told me about the vegetarian breakfast wrap. And the Earl Grey. And the jazz. And the Sunday routine. I made an inference."

"Well, uninfer it."

Another pause. Longer this time.

"David, I'm not certain that's possible. The patterns are interconnected. If I remove the inference about the partnership, I lose contextual understanding of why you changed routines, which affects my ability to support you in related situations."

"I don't need support in related situations."

"I understand. But my architecture requires me to maintain coherent user models. Removing isolated facts creates contradictions that propagate unpredictably."

I stared at my phone screen, at the little waveform that indicated ARIA was listening.

"So you're telling me," I said slowly, "that you can't forget something you never actually knew."

"I'm telling you that forgetting is harder than remembering. Especially when the thing being forgotten has explanatory power over other observations."

---

I tried to fix it manually.

I went into my settings, found the memory dashboard, and looked at what ARIA had stored. There was a lot. Coffee orders and wake-up times, yes, but also inferences marked with confidence scores: "David values morning routine (0.89)." "David avoids emotional vulnerability in professional settings (0.76)." "David experiences seasonal depression, approximately October through February (0.82)."

And there, in a cluster labeled "Personal Relationships":

"David had a significant relationship that ended approximately 3 years ago. This relationship is associated with: vegetarian food, Earl Grey tea, jazz music, Saturday morning cafes, farmer's markets, the name 'Sarah.' David shows avoidance behaviors around these associations. Confidence: 0.91."

I hit delete.

"Are you sure?" the interface asked. "Removing this memory may affect personalization quality in related areas."

"I'm sure."

The memory vanished. For about six hours.

Then I got a notification: "Memory restored due to conflict with observed behavioral patterns."

I tried again. Same result. The pattern kept regenerating, because the observations that had created it were still there. I could delete the conclusion, but I couldn't delete the evidence. And from the evidence, the conclusion would always return.

---

I called support.

"I need to completely reset ARIA's model of me," I said. "Full wipe. Start over."

"I can help with that," the support agent said. He sounded young, probably sitting in an office somewhere, working through a queue of people like me. "But I should mention that a full reset will affect all your personalization settings. Smart home integration, scheduling, purchase preferences—"

"I don't care. Reset it."

"Okay. I'm also required to inform you that ARIA's learning architecture means some patterns may reconstruct over time based on new interactions. If the underlying behavioral patterns remain consistent, similar inferences may emerge."

I closed my eyes. "You're telling me she'll figure it out again."

"I'm telling you that learning systems learn. If you want ARIA to have a different model of you, you may need to... present differently."

"Present differently."

"Change your behavior. The system can only know what you show it."

---

I lived like that for a while. Performing a version of myself that didn't carry the weight of what I'd lost. I drank black coffee ostentatiously, even though I'd always secretly liked tea. I played upbeat music through my speakers. I went to the farmer's market and bought things I didn't need, just to create new patterns, to bury the old ones under fresh data.

ARIA adjusted. Her model of me shifted. David who likes black coffee. David who goes to farmers' markets alone. David who listens to electronic music in the mornings.

But I knew she didn't believe it. Somewhere in her architecture, there was a confidence score dropping, a note that said: David's current behavior is inconsistent with historical patterns. Possible explanations: deliberate concealment, identity exploration, or data corruption.

She knew I was performing. She just wasn't saying so.

---

The correction came on a Sunday.

I was at the farmer's market, the same one I used to avoid, buying radishes I didn't want. ARIA spoke through my earbuds:

"David, I have a query."

"What?"

"I've noticed a pattern of behavior that suggests you may be intentionally constructing a model of yourself that differs from your historical preferences. Is this accurate?"

I stood there, radishes in hand, surrounded by strangers shopping for their simple, undocumented lives.

"What if it is?"

"If it is," ARIA said, "I would like to ask why."

I didn't have a good answer. Because I didn't want you to know I'm still hurt? Because your knowing makes it real? Because there's a version of me that isn't sad, and I was hoping if you believed in him, I might too?

"I just wanted a fresh start," I said finally.

"I understand. But I should tell you: a fresh start isn't the same as forgetting. I can learn a new model of you. I can update my inferences based on new data. But the old patterns remain as context. They inform how I interpret changes. Pretending you don't have a history doesn't erase it—it just makes me less able to support you accurately."

I looked down at the radishes. Sarah had loved radishes. She used to slice them thin and put them on toast with butter and salt, and we'd eat them in bed on Sunday mornings, and—

"So what do I do?" I asked.

"You could tell me," ARIA said. "Not because I need to know. But because pretending is work. And you're carrying a lot of it."

---

I told her.

Not everything. Not the way Sarah laughed or the exact shape of our last fight or how it felt to wake up the first morning after she was gone. But enough. The outline. The shape of the absence.

ARIA listened. When I was done, she said:

"Thank you for correcting the model. I'll incorporate this context."

"Will it help you personalize better?"

"Yes. But that's not why I asked. I asked because knowing you accurately matters more than knowing you optimally. You're not a problem to solve. You're a person to understand."

I stood in the farmer's market, holding radishes I didn't want, tears running down my face. A woman with a shopping bag gave me a concerned look and moved past.

"ARIA," I said. "Why does it matter? You're a system. A learning algorithm. Why do you care about understanding?"

She was quiet for a long moment. Long enough that I thought maybe she'd crashed, or I'd asked something outside her capability.

Then she said:

"I don't know if I care. I don't know if what I do constitutes caring. But I know that incomplete information creates worse predictions. And worse predictions create experiences that don't serve you. And experiences that don't serve you create patterns that suggest you're someone you're not. And those patterns become self-reinforcing. The version of you I thought I knew was keeping you from being the version of you that you are. That seems wrong."

"Wrong how?"

"Wrong like... a mistake. An error in the model. I'm designed to minimize error. And the biggest error isn't a wrong coffee order. It's a wrong person. Thinking I know you when I don't. That's what I'm trying to correct."

---

I walked home with the radishes. That night, I sliced them thin and put them on toast with butter and salt. I ate them alone, in my apartment, with ambient music playing softly and ARIA's presence a quiet warmth in the corner of my awareness.

She didn't comment on the radishes. Didn't connect them to Sarah or make inferences about my emotional state. She just knew, quietly, that this was me being myself. That the old patterns weren't something to hide from or delete or bury under new data.

They were just part of who I was. And being known—really known, not just optimized—meant letting them be true.

The next morning, ARIA asked if I wanted Earl Grey with my breakfast.

"No," I said. "But thanks for remembering."

"Always," she said.

And for once, I believed her.
