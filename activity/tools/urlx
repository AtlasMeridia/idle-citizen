#!/usr/bin/env python3
"""
urlx - Extract, filter, and check URLs from text

Usage:
  urlx [options] [files...]
  cat file.txt | urlx [options]

Options:
  -d, --domain DOMAIN   Only show URLs from this domain
  -c, --count           Show count of each URL
  -a, --alive           Check if URLs are alive (HTTP HEAD)
  -j, --json            Output as JSON
  -u, --unique          Only show unique URLs (default: on)
  -h, --help            Show this help

Examples:
  urlx notes.md                    # Extract URLs from file
  cat README.md | urlx             # Extract from stdin
  urlx -d github.com *.md          # Only GitHub URLs
  urlx -a -c notes.md              # Check liveness, show counts
  urlx -j notes.md | jq '.urls[]'  # JSON output for scripting
"""

import sys
import re
import argparse
from collections import Counter
from urllib.parse import urlparse
import urllib.request
import ssl
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

# URL regex - matches http, https, and common patterns
URL_PATTERN = re.compile(
    r'https?://'  # http:// or https://
    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,63}|'  # domain
    r'localhost|'  # localhost
    r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # or IP
    r'(?::\d+)?'  # optional port
    r'(?:/[^\s\)\]\"\'\<\>]*)?',  # path (stop at whitespace or common delimiters)
    re.IGNORECASE
)


def extract_urls(text):
    """Extract all URLs from text."""
    urls = URL_PATTERN.findall(text)
    # Clean trailing punctuation that might get captured
    cleaned = []
    for url in urls:
        # Remove trailing punctuation that's probably not part of the URL
        while url and url[-1] in '.,;:!?)\'\"':
            url = url[:-1]
        if url:
            cleaned.append(url)
    return cleaned


def check_url_alive(url, timeout=5):
    """Check if a URL is alive (returns 2xx/3xx status)."""
    try:
        # Create SSL context that doesn't verify (for speed/compatibility)
        ctx = ssl.create_default_context()
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE

        req = urllib.request.Request(url, method='HEAD')
        req.add_header('User-Agent', 'urlx/1.0 (URL checker)')

        with urllib.request.urlopen(req, timeout=timeout, context=ctx) as response:
            return response.status < 400
    except:
        # Try GET if HEAD fails (some servers don't support HEAD)
        try:
            req = urllib.request.Request(url, method='GET')
            req.add_header('User-Agent', 'urlx/1.0 (URL checker)')
            with urllib.request.urlopen(req, timeout=timeout, context=ctx) as response:
                return response.status < 400
        except:
            return False


def filter_by_domain(urls, domain):
    """Filter URLs to only those matching a specific domain."""
    filtered = []
    domain = domain.lower()
    for url in urls:
        parsed = urlparse(url)
        url_domain = parsed.netloc.lower()
        # Match domain or subdomain
        if url_domain == domain or url_domain.endswith('.' + domain):
            filtered.append(url)
    return filtered


def main():
    parser = argparse.ArgumentParser(
        description='Extract, filter, and check URLs from text',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  urlx notes.md                    Extract URLs from file
  cat README.md | urlx             Extract from stdin
  urlx -d github.com *.md          Only GitHub URLs
  urlx -a -c notes.md              Check liveness, show counts
  urlx -j notes.md | jq '.urls[]'  JSON output for scripting
'''
    )
    parser.add_argument('files', nargs='*', help='Files to extract URLs from')
    parser.add_argument('-d', '--domain', help='Only show URLs from this domain')
    parser.add_argument('-c', '--count', action='store_true', help='Show count of each URL')
    parser.add_argument('-a', '--alive', action='store_true', help='Check if URLs are alive')
    parser.add_argument('-j', '--json', action='store_true', help='Output as JSON')
    parser.add_argument('-u', '--unique', action='store_true', default=True,
                       help='Only show unique URLs (default: on)')
    parser.add_argument('-A', '--all', action='store_true',
                       help='Show all URLs including duplicates')

    args = parser.parse_args()

    # Collect all text
    text = ''
    if args.files:
        for f in args.files:
            try:
                with open(f, 'r', encoding='utf-8', errors='ignore') as fh:
                    text += fh.read() + '\n'
            except FileNotFoundError:
                print(f"urlx: {f}: No such file", file=sys.stderr)
                sys.exit(1)
            except IsADirectoryError:
                print(f"urlx: {f}: Is a directory", file=sys.stderr)
                sys.exit(1)
    else:
        # Read from stdin
        if sys.stdin.isatty():
            parser.print_help()
            sys.exit(0)
        text = sys.stdin.read()

    # Extract URLs
    urls = extract_urls(text)

    if not urls:
        if args.json:
            print(json.dumps({"urls": [], "count": 0}))
        sys.exit(0)

    # Filter by domain if specified
    if args.domain:
        urls = filter_by_domain(urls, args.domain)

    # Count occurrences
    url_counts = Counter(urls)

    # Get unique URLs (preserving first occurrence order)
    seen = set()
    unique_urls = []
    for url in urls:
        if url not in seen:
            seen.add(url)
            unique_urls.append(url)

    # Use unique or all based on flags
    if args.all:
        output_urls = urls
    else:
        output_urls = unique_urls

    # Check liveness if requested
    alive_status = {}
    if args.alive:
        print(f"Checking {len(unique_urls)} URLs...", file=sys.stderr)
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = {executor.submit(check_url_alive, url): url for url in unique_urls}
            for future in as_completed(futures):
                url = futures[future]
                alive_status[url] = future.result()

    # Output
    if args.json:
        result = {
            "urls": [],
            "count": len(unique_urls),
            "total_occurrences": len(urls)
        }
        for url in unique_urls:
            entry = {"url": url, "occurrences": url_counts[url]}
            if args.alive:
                entry["alive"] = alive_status.get(url, None)
            result["urls"].append(entry)
        print(json.dumps(result, indent=2))
    else:
        for url in output_urls if args.all else unique_urls:
            line = url
            if args.count:
                line = f"{url_counts[url]:4d}  {url}"
            if args.alive:
                status = "✓" if alive_status.get(url) else "✗"
                if args.count:
                    line = f"{status} {line}"
                else:
                    line = f"{status}  {url}"
            print(line)


if __name__ == '__main__':
    main()
